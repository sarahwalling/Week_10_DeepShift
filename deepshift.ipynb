{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbMiwUjQr9Yu"
   },
   "source": [
    "# Part 1: Simple DeepShift-Q\n",
    "\n",
    "First off we'll go through my first attempt at implementing something like DeepShift-Q. Although it seems to work, it's a bit crude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDvVGma3r9Yv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "n_epochs=5\n",
    "lr=1.0\n",
    "gamma=0.7\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "\n",
    "kwargs = {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsZMvIqRr9Yy"
   },
   "source": [
    "# Baseline\n",
    "Let's establish a baseline for \"normal\" model performance. We'll train the pytorch example CNN on mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIYOwOUpr9Yz"
   },
   "outputs": [],
   "source": [
    "class BaselineNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vcyJlryr9Y1"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        readout = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item())\n",
    "        sys.stdout.write(readout)\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write('\\r')\n",
    "        \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "RiOEWwfyr9Y4",
    "outputId": "510b4eda-0749-4c06-90ff-294c079ae8d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 0.027809\n",
      "Test set: Average loss: 0.0484, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: 0.014486\n",
      "Test set: Average loss: 0.0363, Accuracy: 9881/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: 0.009929\n",
      "Test set: Average loss: 0.0346, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: 0.065454\n",
      "Test set: Average loss: 0.0310, Accuracy: 9898/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [29984/60000 (100%)]\tLoss: 0.058990\n",
      "Test set: Average loss: 0.0270, Accuracy: 9904/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BaselineNet().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FDUooJlUr9Y7"
   },
   "source": [
    "# Rounding to powers of 2\n",
    "\n",
    "Now let's define some functions to go about quantizing weights to their nearest power of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tAC9kLFCr9Y8"
   },
   "outputs": [],
   "source": [
    "def get_shift_and_sign(x, rounding='deterministic'):\n",
    "    # TODO: get the sign and the absolute value of x\n",
    "    sign = np.sign(x)\n",
    "    x_abs = np.abs(x)\n",
    "    \n",
    "    # TODO: now how do we get the shift parameter?\n",
    "    shift = round(np.log2(x_abs), rounding)\n",
    "    return (shift, sign)\n",
    "\n",
    "def round(x, rounding='deterministic'):\n",
    "    # we can experiment with stochastic or deterministic rounding\n",
    "    assert(rounding in ['deterministic', 'stochastic'])\n",
    "    if rounding == 'stochastic':\n",
    "        x_floor = x.floor()\n",
    "        return x_floor + torch.bernoulli(x - x_floor)\n",
    "    else:\n",
    "        return x.round()\n",
    "\n",
    "def round_power_of_2(x, rounding='deterministic'):\n",
    "    # TODO: we want this function to round to x to the nearest power of 2\n",
    "    shift, sign = get_shift_and_sign(x)\n",
    "    \n",
    "    # TODO: once we have the shift and sign, how do we get the rounded version of x?\n",
    "    x_rounded = (2 ** shift) * sign\n",
    "    return x_rounded\n",
    "    \n",
    "def quantize_power_of_2(layer):\n",
    "    # we'll use this function to directly modify the weights\n",
    "    for param in layer.parameters():\n",
    "        # TODO: use our rounding function to quantize the parameters\n",
    "        param.data = round_power_of_2(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7o0oyZjr9Y-"
   },
   "outputs": [],
   "source": [
    "class CrudeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrudeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        quantize_power_of_2(self.conv1)\n",
    "        quantize_power_of_2(self.conv2)\n",
    "        quantize_power_of_2(self.fc1)\n",
    "        quantize_power_of_2(self.fc2)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "ZyAuILi_r9ZD",
    "outputId": "d0f221cc-6e14-465e-841c-a5d3b488109f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 0.063619\n",
      "Test set: Average loss: 0.1703, Accuracy: 9561/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: 0.171254\n",
      "Test set: Average loss: 0.1512, Accuracy: 9606/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: 0.244177\n",
      "Test set: Average loss: 0.1500, Accuracy: 9618/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: 0.321546\n",
      "Test set: Average loss: 0.1501, Accuracy: 9614/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [29984/60000 (100%)]\tLoss: 0.289334\n",
      "Test set: Average loss: 0.1501, Accuracy: 9614/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\") #I can't use cuda :( \n",
    "model = CrudeNet().to(device)\n",
    "\n",
    "# we'll use SGD for this one, since the authors claimed\n",
    "# that SGD works the best for DeepShift-Q\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: \n",
    "Why are we able to get away with crudely quantizing the layers like this without explicitly messing with the gradient?  \n",
    "Autograd doesn't use our quantization (before the forward pass) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycsWmugDr9ZG"
   },
   "source": [
    "# Part 2: Implementing DeepShift-PS\n",
    "\n",
    "It really does seem like our crude quantization works well enough, but the way it is implemented leaves a bad taste in my mouth. Now we'll implement DeepShift-PS in a fashion that is maybe a bit more satisfying. We will need to get a bit more clever to do this. Before starting, take a look at ste.py and answer the following question.\n",
    "\n",
    "# Question:\n",
    "How are the straight-through estimators implemented in ste.py? Why do these functions give us the desired behavior? (Hint: think back to when we looked at autograd)\n",
    "They're Torch functions - which autograd can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BeKG4XcJr9ZG"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "from torch.nn.modules.utils import _pair\n",
    "from torch.nn import init\n",
    "\n",
    "import utils as utils\n",
    "import ste as ste # ***if you're running in google colab, you'll need to manually import this to the runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWrFwKCgr9ZI"
   },
   "source": [
    "# Implementing a bitshift linear layer\n",
    "\n",
    "We need to make a custom pytorch layer to get the desired implementation. You might want to take a look at https://pytorch.org/docs/stable/notes/extending.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi7InUzqr9ZJ"
   },
   "outputs": [],
   "source": [
    "class LinearShift(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, freeze_sign=False, use_cuda=True, \n",
    "                 rounding='deterministic', weight_bits=5):\n",
    "        \n",
    "        super(LinearShift, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_cuda = use_cuda\n",
    "        self.rounding = rounding\n",
    "        self.shift_range = (-1 * (2**(weight_bits - 1) - 2), 0) \n",
    "        \n",
    "        # nn.Parameter is a special kind of Tensor, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        \n",
    "        # here we register the shift (P) and sign (S) as parameters\n",
    "        self.shift = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.sign = nn.Parameter(torch.Tensor(out_features, in_features), requires_grad = (freeze_sign == False))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.shift.data.uniform_(*self.shift_range) \n",
    "        self.sign.data.uniform_(-1, 1) \n",
    "        \n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.shift)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # force P into a predefined range\n",
    "        self.shift.data = ste.clamp(self.shift.data, *self.shift_range)\n",
    "        \n",
    "        # Hint: use the straight-through estimator functions for this part     \n",
    "        # TODO: make a temporary rounded version of P\n",
    "        shift_rounded = ste.round(self.shift)\n",
    "        \n",
    "        # TODO: make a temproary rounded version of S\n",
    "        sign_rounded_signed = ste.sign(ste.round(self.sign))\n",
    "        \n",
    "        # TODO: make a temporary weight matrix built from P and S\n",
    "        weight_ps = ste.unsym_grad_mul(2**shift_rounded, sign_rounded_signed)\n",
    "    \n",
    "        # with all that said and done, we can just pass W_ps to a vanilla\n",
    "        # pytorch linear layer\n",
    "\n",
    "        return torch.nn.functional.linear(input, weight_ps, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oAe8Fe2Or9ZL"
   },
   "source": [
    "# Implementing a bitshift convolutional layer\n",
    "\n",
    "Now that we have our linear layer, we'll make a convolutional layer. This will be similar to what we just did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ecxwmq7r9ZM"
   },
   "outputs": [],
   "source": [
    "class Conv2dShift(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1,# output_padding,\n",
    "                 groups=1, bias=True, padding_mode='zeros', \n",
    "                 rounding='deterministic', weight_bits=5):\n",
    "        \n",
    "        super(Conv2dShift, self).__init__()\n",
    "        \n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        \n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        self.rounding=rounding\n",
    "        self.shift_range = (-1 * (2**(weight_bits - 1) - 2), 0)\n",
    "\n",
    "        self.shift = nn.Parameter(torch.Tensor(\n",
    "            out_channels, in_channels // groups, *kernel_size))\n",
    "        self.sign = nn.Parameter(torch.Tensor(\n",
    "            out_channels, in_channels // groups, *kernel_size),\n",
    "            requires_grad = True)\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.shift.data.uniform_(-10, -1) # (-0.1, 0.1)\n",
    "        self.sign.data.uniform_(-1, 1) # (-0.1, 0.1)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.shift)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # force P into a predefined range\n",
    "        self.shift.data = ste.clamp(self.shift.data, *self.shift_range)\n",
    "        \n",
    "        # we'll rely on our straight-through estimator functions again here\n",
    "        # TODO: fill these out\n",
    "        shift_rounded = ste.round(self.shift)\n",
    "        \n",
    "        sign_rounded_signed = ste.sign(ste.round(self.sign))\n",
    "        \n",
    "        weight_ps = ste.unsym_grad_mul(2**shift_rounded, sign_rounded_signed)\n",
    "        \n",
    "        if self.padding_mode == 'circular':\n",
    "            expanded_padding = ((self.padding[1] + 1) // 2, self.padding[1] // 2,\n",
    "                                (self.padding[0] + 1) // 2, self.padding[0] // 2)\n",
    "\n",
    "            input_padded = F.pad(input, expanded_padding, mode='circular')\n",
    "            padding =  _pair(0)\n",
    "        else:\n",
    "            input_padded = input\n",
    "            padding = self.padding\n",
    "            \n",
    "        return torch.nn.functional.conv2d(input_padded, weight_ps, self.bias, \n",
    "                                          self.stride, padding, self.dilation, self.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhCzP2gor9ZO"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = Conv2dShift(1, 32, 3, 1)\n",
    "        self.conv2 = Conv2dShift(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = LinearShift(9216, 128)\n",
    "        self.fc2 = LinearShift(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "colab_type": "code",
    "id": "Lb-YOclGr9ZT",
    "outputId": "1113aaf0-7d47-4ff6-fa0d-e6cb3de2827c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 0.861998\n",
      "Test set: Average loss: 0.3863, Accuracy: 9042/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: 0.384131\n",
      "Test set: Average loss: 0.2528, Accuracy: 9315/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: 0.423262\n",
      "Test set: Average loss: 0.2116, Accuracy: 9403/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: 0.383833\n",
      "Test set: Average loss: 0.1864, Accuracy: 9464/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [29984/60000 (100%)]\tLoss: 0.275506\n",
      "Test set: Average loss: 0.1800, Accuracy: 9480/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs=5\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final thoughts\n",
    "\n",
    "I had a hard time getting comparable accuracy to the baseline with this, but play around with it and see what you can come up with.\n",
    "\n",
    "As it is here, my accuracy is a *little* lower, but still quite good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "shiftnet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
